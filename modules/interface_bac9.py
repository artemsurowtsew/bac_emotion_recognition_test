# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'C:\Users\Artem\PycharmProjects\pythonProject\modules\interface_bac5.ui'
#
# Created by: PyQt5 UI code generator 5.15.9
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.


from PyQt5 import QtCore, QtGui, QtWidgets
import cv2
import mss
import numpy as np
from tensorflow.keras.models import model_from_json
from PyQt5.QtGui import QImage, QPixmap

json_file = open(r'C:\Users\Artem\PycharmProjects\pythonProject\model\emotion_model3.json')
loaded_model_json = json_file.read()
json_file.close()
emotion_model = model_from_json(loaded_model_json)

# Завантаження ваг з H5-файлу
emotion_model.load_weights(r"C:\Users\Artem\PycharmProjects\pythonProject\model\emotion_model3.h5")

print("Loaded model architecture and weights from disk")

# Load Haar cascade for face detection
face_cascade = cv2.CascadeClassifier(
    r'C:\Users\Artem\PycharmProjects\pythonProject\haarcascades/haarcascade_frontalface_default.xml')


class Ui_EmotionRecognition(object):
    def setupUi(self, EmotionRecognition):
        EmotionRecognition.setObjectName("EmotionRecognition")
        EmotionRecognition.resize(1082, 887)
        EmotionRecognition.setStyleSheet("#tabWidget{\n"
                                         "background-color: #FFFFFF\n"
                                         "}\n"
                                         "\n"
                                         "QPushButton {\n"
                                         "    background-color: #ff7f24; /* Оранжевий колір фону кнопки */\n"
                                         "    color: #000000; /* Сірий колір тексту кнопки */\n"
                                         "}\n"
                                         "\n"
                                         "QLineEdit {\n"
                                         "    background-color: #D3D3D3; /* Світло-сірий колір фону поля введення */\n"
                                         "    color: #FFA500; /* Оранжевий колір тексту поля введення */\n"
                                         "}")
        self.centralwidget = QtWidgets.QWidget(EmotionRecognition)
        self.centralwidget.setObjectName("centralwidget")
        self.screen_thread = QtWidgets.QLabel(self.centralwidget)
        self.screen_thread.setGeometry(QtCore.QRect(20, 0, 600, 480))
        self.screen_thread.setMinimumSize(QtCore.QSize(600, 400))
        self.screen_thread.setMaximumSize(QtCore.QSize(640, 480))
        self.screen_thread.setFrameShape(QtWidgets.QFrame.NoFrame)
        self.screen_thread.setText("")
        self.screen_thread.setObjectName("screen_thread")
        self.emotions_screen_reg = QtWidgets.QTextEdit(self.centralwidget)
        self.emotions_screen_reg.setGeometry(QtCore.QRect(20, 760, 760, 80))
        self.emotions_screen_reg.setMinimumSize(QtCore.QSize(760, 20))
        self.emotions_screen_reg.setMaximumSize(QtCore.QSize(211, 311))
        self.emotions_screen_reg.setObjectName("emotions_screen_reg")
        self.labelscreen1 = QtWidgets.QLabel(self.centralwidget)
        self.labelscreen1.setGeometry(QtCore.QRect(40, 730, 171, 21))
        font = QtGui.QFont()
        font.setPointSize(14)
        self.labelscreen1.setFont(font)
        self.labelscreen1.setObjectName("labelscreen1")
        self.load_screen_button = QtWidgets.QPushButton(self.centralwidget)
        self.load_screen_button.setGeometry(QtCore.QRect(800, 770, 200, 70))
        self.load_screen_button.setObjectName("load_screen_button")
        self.camera_thread = QtWidgets.QLabel(self.centralwidget)
        self.camera_thread.setGeometry(QtCore.QRect(700, 40, 370, 430))
        self.camera_thread.setMinimumSize(QtCore.QSize(100, 100))
        self.camera_thread.setMaximumSize(QtCore.QSize(640, 480))
        self.camera_thread.setFrameShape(QtWidgets.QFrame.NoFrame)
        self.camera_thread.setText("")
        self.camera_thread.setObjectName("camera_thread")
        EmotionRecognition.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(EmotionRecognition)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 1082, 22))
        self.menubar.setObjectName("menubar")
        EmotionRecognition.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(EmotionRecognition)
        self.statusbar.setObjectName("statusbar")
        EmotionRecognition.setStatusBar(self.statusbar)

        self.retranslateUi(EmotionRecognition)
        QtCore.QMetaObject.connectSlotsByName(EmotionRecognition)

        self.timer = QtCore.QTimer()
        self.timer.timeout.connect(self.capture_screen)
        self.timer.start(24)  # Захват кожні 24 мс

        self.camera_timer = QtCore.QTimer()
        self.camera_timer.timeout.connect(self.update_camera_image)
        self.camera_timer.start(30)  # Оновлення камери кожні 30 мс

        self.cap = cv2.VideoCapture(0)

    def retranslateUi(self, EmotionRecognition):
        _translate = QtCore.QCoreApplication.translate
        EmotionRecognition.setWindowTitle(_translate("EmotionRecognition", "MainWindow"))
        self.labelscreen1.setText(_translate("EmotionRecognition", "Emotions Detected"))
        self.load_screen_button.setText(_translate("EmotionRecognition", ""))

    def capture_screen(self):
        # Захват экрана
        with mss.mss() as sct:
            monitor = {"top": 0, "left": 0, "width": 800, "height": 600}
            img = np.array(sct.grab(monitor))

            # Обработка изображения и распознавание эмоций
        self.process_frame(img)

        # Update the QLabel with the processed image
        self.update_image(img)

    def process_frame(self, frame):
        # Преобразование изображения в оттенки серого для обнаружения лиц
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Обнаружение лиц
        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

        # Обработка каждого обнаруженного лица
        for (x, y, w, h) in faces:
            face_gray = gray_frame[y:y + h, x:x + w]  # Используем оттенки серого для обработки
            face_color = frame[y:y + h, x:x + w]  # Используем цветное изображение для отображения
            emotion_labels = ["Angry", "Disgusted", "Fearful", "Happy", "Neutral", "Sad", "Surprised"]
            # Обработка face_gray с помощью вашей модели            emotion_labels = ["Angry", "Disgusted", "Fearful", "Happy", "Neutral", "Sad", "Surprised"]
            face_gray_resized = cv2.resize(face_gray, (48, 48))
            face_gray_normalized = face_gray_resized / 255.0
            face_gray_reshaped = np.reshape(face_gray_normalized, (1, 48, 48, 1))
            predicted_emotion = emotion_model.predict(face_gray_reshaped)
            predicted_emotion_label = emotion_labels[np.argmax(predicted_emotion)]

    def update_image(self, img):
        # Convert the image format to RGB for QPixmap
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        qimg = QtGui.QImage(img_rgb.data, img_rgb.shape[1], img_rgb.shape[0], QtGui.QImage.Format_RGB888)
        pixmap = QtGui.QPixmap.fromImage(qimg)
        self.screen_thread.setPixmap(pixmap)

    def update_camera_image(self):
        ret, frame = self.cap.read()
        if ret:
            # Обробка зображення з камери
            self.web_process_frame(frame)

    def web_process_frame(self, frame):
        # Преобразование изображения в оттенки серого для обнаружения лиц
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # Обнаружение лиц
        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
        # Ініціалізація тексту для QTextEdit
        emotions_text = ""
        # Обработка каждого обнаруженного лица
        for (x, y, w, h) in faces:
            face_gray = gray_frame[y:y + h, x:x + w]
            # ... Обработка face_gray с помощью вашей модели ...
            emotion_labels = ["Angry", "Disgusted", "Fearful", "Happy", "Neutral", "Sad", "Surprised"]
            face_gray_resized = cv2.resize(face_gray, (48, 48))
            face_gray_normalized = face_gray_resized / 255.0
            face_gray_reshaped = np.reshape(face_gray_normalized, (1, 48, 48, 1))
            predicted_emotion = emotion_model.predict(face_gray_reshaped)
            predicted_emotion_label = emotion_labels[np.argmax(predicted_emotion)]
            # Додавання розпізнаної емоції до тексту
            emotions_text += predicted_emotion_label + "\n"
            # Отображение результатов на face_color
            cv2.putText(frame, predicted_emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)
            # Виведення розпізнаних емоцій в QTextEdit
        self.emotions_screen_reg.setText(emotions_text)
        # Конвертація зображення для відображення у QLabel
        rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb_image.shape
        bytes_per_line = ch * w
        convert_to_Qt_format = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)
        p = convert_to_Qt_format.scaled(640, 480, QtCore.Qt.KeepAspectRatio)
        self.camera_thread.setPixmap(QPixmap.fromImage(p))

    def closeEvent(self, event):
        self.cap.release()
        event.accept()


if __name__ == "__main__":
    import sys

    app = QtWidgets.QApplication(sys.argv)
    EmotionRecognition = QtWidgets.QMainWindow()
    ui = Ui_EmotionRecognition()
    ui.setupUi(EmotionRecognition)
    EmotionRecognition.show()
    sys.exit(app.exec_())